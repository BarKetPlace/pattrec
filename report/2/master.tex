\documentclass[a4paper]{report}

\usepackage[utf8]{inputenc} %Accent
%\usepackage{libertine} %Font
\usepackage[english, francais]{babel} %langue

\usepackage{graphicx} %Include fig
\usepackage{caption} %center the caption
\usepackage{subfig} %Include subfig
\usepackage{lastpage} %ref LastPage 
\usepackage{fancyhdr} % headers,footers
\usepackage{multicol} % minipages
\usepackage{textcomp} 
\usepackage{lscape}   %Format paysage
\usepackage{fancybox} %Image arrière plan
\usepackage{amsmath} %\mathbb, \mathit...
\usepackage{amssymb} 
\usepackage{color} %couleurs
\usepackage{float}
\usepackage[hidelinks]{hyperref} %Liens intradoc et url
\usepackage{titlepic}

%\usepackage{algorithm}
%\usepackage{algorithmic} %Algo en pseudo code
%\usepackage{algorithm2e} %for psuedo code

%\usepackage{boxedminipage} %Surligner

%\newcounter{apppage} % Annexes

%Dossier contenant les figures
\graphicspath{{figures/}}

%Mise en page
\voffset -1.5 cm
\textheight 24.3 cm
\topmargin 0 cm
\headheight 0 cm
%\headsep 0.6 cm
\textwidth 16.5 cm
\evensidemargin 0 cm
\marginparsep 0 cm
\marginparwidth 0 cm
\oddsidemargin -.5 cm

%Titre
\title{Pattern recognition\\Report\\Assignment n°1}
\author{Audrey Brouard \and Antoine Honoré}



%Type de numérotation des sections & sous-sections
\renewcommand{\thesection}{\Roman{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}

%\renewcommand\thesubfigure{(\alph{subfigure})}
\setlength{\parindent}{0cm}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}
\newcommand{\hsp}{\hspace{20pt}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

%email
\newcommand{\email}[1]{\href{mailto:#1}{\color{blue} \textsf{#1}}}

%Bibliography
\bibliographystyle{apalike}

%Environnement insersion image
\newcommand{\img}[3]{\begin{figure}[!h] \centering \includegraphics[scale=#2]{#1}\captionsetup{justification=centering} \caption{#3} \label{#1} \end{figure}}
  % commande \img{nom image}{scale}{legende}

%TODO
\newcommand{\todo}[1]{{ \Large \textbf{ \colorbox{yellow}{\color{blue} TODO:}}~#1}}

%pushright
\newenvironment{pushright}[1]{\textbf{#1}
\begin{itemize}\item[\hspace{12pt}]}{\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}  % Activation en-tête et pied de page

%En-tête
\fancyhead[L]{Audrey Brouard \& Antoine Honoré - Pattern recognition - Assignment n°2}
%\fancyhead[C]{}
%\fancyhead[R]{}
% Pied de page
\newcommand{\width}{3cm}
%\fancyfoot[L]{ \includegraphics[width=\width]{logo-gipsa} }
\fancyfoot[C]{ \thepage~/~\pageref{LastPage} }
%\fancyfoot[R]{ \includegraphics[width=\width]{logo-phelma} }

\titlepic{\includegraphics[scale=0.6]{kth-logo}}

\begin{document}

%%%%%%%%%%%%%%%% TITLE %%%%%%%%%%%%%%%%
\input{title.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{}


1. Below are represented the pitch and intensity profiles of the 3 songs provided. 
\img{Pitch_intens}{.8}{Pitches and Intensity for the 3 melodies}

On the pitch profile we can notice that that's we hardly see the different "useful" pitches as the noise has a pitch of around 1000Hz. The important thing here is that the pitch reaches 1000Hz with a very low intensity. Which means that a threshold on intensity should be enough to get rid of the noise. If we zoom in we can clearly read the pitch values corresponding to different notes :

\img{Pitch_intens_zoom}{.8}{Zoom}

Here on the pitch profile we can distinguish different values of pitch between 100Hz and 300Hz, which corresponds to different notes, globally spread over 1.5 octave.

\section{Design of the feature extractor}
\subsection{Our decisions}
In order to fulfil all the conditions listed in the subject, we designed our feature extractor as follows :
\begin{itemize}
\item First we extract the actual pitches by removing the noise around 1000Hz so that we only have the pitches containing a lot of information.

\item Then we interpolate the vector thus obtained with a vector containing all the notes in the range 20Hz-20kHz. This last vector was built thanks to the constant ratio between 2 consecutive semitons. This way we obtain a vector containing the different notes played in the song, but so far we are dependant of the octave.

\item To that purpose, we set a reference to remove the offset of the melody. The reference is defined as the first pitch which is not computed during a silence. After that, we are able to determine how many semitons above or below the reference the next pitches are located. We thus obtain a vector containing the variation of semiton from the reference for each pitch, and infinity (or NaN) when it's a silence.

\item Finally, the duration of each note/silence will be determined by the number of time the source remains in the same state. To avoid temporal distorsion issues (a singer might sing faster or slower than another one), we will define a vector containing the ratio between two notes durations.
\end{itemize}

\subsection{Integration to PattRecClasses}
The states correspond to the semitons offset. Let us consider an example : we extract such features $[0 4 4 4 4 0 -1 -1 \dots]$. This vector is a vector of frames and here the first one is the reference. The five following fours mean that this part of the signal has a pitch 4 semitons above the reference. The states come naturally to be integers representing an offset of semitons. The actual value of pitches in frame 2 to 5 are not the same, their distribution defines the distribution of state 4.
The extraction of the offset is made in the function find\_offset.

\subsection{Example}
Let us now consider a real example. Melody 1 and melody 2 are the same melody sung at a different level. Besides, the two songs do not have the same duration.
To perform the test we followed these steps: \begin{itemize}
\item Load the two songs;
\item Run GetMusicFeatures with the default parameters on the two signals;
\item Run find\_offset on the two signals and get $v_{1}~\&~v_{2}$, which represent the vectors of offset that we discussed about in the previous section.

the plot of $v_{1}~\&~v_{2}$ is shown on figure \ref{test_extractor}.
\end{itemize}
\img{test_extractor}{.45}{ {\color{blue}Blue: melody 1} {\color{red}Red: melody 2}}
On the figure \ref{test_extractor}, we clearly see that the serie of semiton is the same. In the first melody, the singer hold the notes longer than in the second.


On the contrary if we now had to the graphics the third melody (figure \ref{Comparison_3_songs}), which is different from the 2 others, we can clearly see that the behaviour is completely different. 
\img{Comparison_3_songs}{0.5}{Black: third melody, different from the  1 others\\The black track decreases only at the end of the song}

As the song sung is different, the notes are different too which explains why the offset obtained varies. This is the behaviour we expected from our feature extractor.

\pagebreak
\subsection{Weaknesses of our feature extractor}

So far, our feature extractor can recognize a song sung even if it's transposed, thanks to the offset. It can also recognize the song even if it's sung slower or quicker than the original. However, for the moment our feature extractor doesn't take into account the ambient noise which can influence the pitch of the notes played.
We tried to reproduce the melody 3 with our own voice. The result is shown on figure \ref{melody3_experience}.

\img{melody3_experience}{.3}{{\color{blue}Blue: Melody 3}, {\color{red}Red: our own voice}}
The feature extractor suffers from a lack of precision. It is very difficult to reproduce to same tracks of offset in normal\footnote{Means with the microphone of a laptop and without being an excellent singer} recording conditions. On figure \ref{melody3_experience} \& \ref{test_extractor}, there is still a constant difference between the to tracks of semitons. Our reference was supposed to tackle this issue. The choice of the reference is to be reconsidered but so far we didn't come up with a better solution.
\end{document}
